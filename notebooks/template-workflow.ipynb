{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU openai marvin\n",
    "%pip install -qU \"psycopg[binary]\"\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ready\n",
    "\n",
    "Begin by:\n",
    "1. creating a new directory `data/<jurisdiction>` and populate with one or more\n",
    "docx files containing the jurisdiction's municipal code\n",
    "2. run `scripts/convert_docx.sh` to convert those files into a single text file\n",
    "3. make a copy of `notebooks/template-workflow.ipynb` to `notebooks/<jurisdiction>.ipynb`\n",
    "and continue processing in that notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "## set up auto-reloading for development\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify heading patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the `jurisdiction_headings` dict with examples from your jurisdiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muni.code import Jurisdiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.embeddings.create(\n",
    "  input=\"It was the best of times, it was the worst of times.\",\n",
    "  model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "print(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_examples = {\n",
    "    1: [\"TITLE 1\\nGENERAL PROVISION\\n\",\n",
    "        \"TITLE 2\\nCITY GOVERNMENT AND ADMINISTRATION\\n\",\n",
    "        \"TITLE 3\\nREVENUE AND FINANCE\\n\",\n",
    "        ],\n",
    "    2: [\"CHAPTER 1-4\\nCODE ADOPTION - ORGANIZATION\\n\",\n",
    "        \"CHAPTER 1-8\\nCITY SEAL AND FLAG\\n\",\n",
    "        \"CHAPTER 1-12\\nCITY EMBLEMS\\n\",\n",
    "        ],\n",
    "    3: [\"1-4-010 Municipal Code of Chicago adopted.\\n\",\n",
    "        \"2-1-020 Code to be kept up-to-date.\\n\",\n",
    "        \"3-4-030 Official copy on file.\\n\",\n",
    "        ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muni.code import infer_heading_patterns, infer_level_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify that the regular expressions matching outline levels look okay\n",
    "heading_patterns = infer_heading_patterns(heading_examples)\n",
    "for level, pattern in heading_patterns.items():\n",
    "    print(f\"{level}: r'{pattern.regex}'\")\n",
    "\n",
    "print()\n",
    "\n",
    "## Verify that the names of the sections look okay\n",
    "level_names = infer_level_names(heading_patterns)\n",
    "for level, name in level_names.items():\n",
    "    print(f\"{level}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the parameters of the jurisdiction and parse the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "place = Jurisdiction(\n",
    "    name=\"Chicago Mini\",\n",
    "    title=\"Municipal Code of Chicago\",\n",
    "    patterns=heading_patterns,\n",
    "    level_names=level_names,\n",
    "    source_local=\"../data/chicago-mini/code.txt\",\n",
    "    source_url=\"https://www.chicago.gov/city/en/depts/doit/supp_info/municipal_code.html\",\n",
    ")\n",
    "\n",
    "place.parse()\n",
    "place.chunkify(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify that the distribution of paragraphs and chunks looks okay\n",
    "place.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muni.code import upload\n",
    "\n",
    "db = {'dbname': 'muni',\n",
    "      'user': 'muni',\n",
    "      'password': '',\n",
    "      'host': 'localhost',\n",
    "      'port': 5432}\n",
    "\n",
    "upload(db, place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muni.code import upload_embeddings, refresh_views\n",
    "\n",
    "upload_embeddings(db, place)\n",
    "refresh_views(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find associations among sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muni.code import find_associations\n",
    "\n",
    "find_associations(db, place)\n",
    "# TODO: changing DB schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muni.code import simple_semantic_query\n",
    "\n",
    "queries = ['Does the municipal code contain provisions restricting the use of drug paraphernalia?']\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    results = simple_semantic_query(db, place, query, limit=20)\n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muni.code import extract_keywords, simple_full_text_query\n",
    "\n",
    "## FIXME: this doesn't work well because extract_keywords() returns too many keywords\n",
    "# queries = [' '.join(extract_keywords(query)) for query in queries]\n",
    "\n",
    "queries = ['drug paraphernalia']\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    results = simple_full_text_query(db, place, query, limit=20)\n",
    "    for result in results:\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muni.code import hybrid_query\n",
    "\n",
    "queries = ['Does the municipal code contain provisions restricting the use of drug paraphernalia?']\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    results = hybrid_query(db, place, query, limit=20)\n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muni.code import Report\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "query = 'Does the municipal code contain provisions restricting the use of drug paraphernalia?'\n",
    "\n",
    "report = Report(db, place, query)\n",
    "\n",
    "display(Markdown(str(report)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload results to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muni.code import upload_report\n",
    "\n",
    "upload_report(db, report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [ ] #Set of queries seperated by comma (,)\n",
    "\n",
    "gt_labels = [ ] #Ground Truth Labels - 0 for No and 1 for Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualititve Analysis\n",
    "\n",
    "This analysis measures model performance using objective metrics such as accuracy. It includes comparing generated outputs against ground truth labels(gt_lables) to evaluate correctness and reliability at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muniscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name   = \"CITY NAME\"          # <-- pick the city you want\n",
    "juris       = place         # the Jurisdiction obj you already built\n",
    "gt_labels   = gt_labels # <-- 0=no, 1=yes for each query\n",
    "\n",
    "# The nine DPL priority questions in order:\n",
    "queries = queries\n",
    "\n",
    "def yn_to_int(ans):\n",
    "    \"\"\"\n",
    "    yes  → 1\n",
    "    no   → 0\n",
    "    None → 0   # <- treat missing answer as 'no'\n",
    "    \"\"\"\n",
    "    if ans is None:\n",
    "        return 0\n",
    "    return 1 if ans.strip().lower().startswith(\"y\") else 0\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#   RUN\n",
    "# NA - for no answer?\n",
    "#  \n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(f\"Results for {city_name}:\\n\")\n",
    "correct = 0\n",
    "for i, q in enumerate(queries):\n",
    "    report = Report(db, juris, q)          # runs retrieval + QA\n",
    "    pred   = yn_to_int(report.short_answer)       # now never raises\n",
    "    truth  = gt_labels[i]\n",
    "\n",
    "    if report.short_answer is None:\n",
    "        disp = \"—\"            # show dash for “no answer”\n",
    "    else:\n",
    "        disp = report.short_answer\n",
    "\n",
    "    if pred == truth:\n",
    "        correct += 1\n",
    "    print(f\"{i+1}. Model: {disp}  |  Truth: {truth}  \"\n",
    "          f\"{'✓' if pred==truth else '✗'}\")\n",
    "\n",
    "accuracy = correct / len(queries)\n",
    "print(f\"\\nAccuracy: {accuracy:.2%}  ({correct}/{len(queries)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gpt-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()  \n",
    "\n",
    "queries_x = [ ] \n",
    "\"\"\"Use a set of queries, but include the city’s name in each query to ensure GPT-4o interprets them correctly.\n",
    "Example:\n",
    "For Chicago_IL: ‘Are there exemptions specifically for fentanyl testing or checking equipment?\"\"\"\n",
    "\n",
    "gt_labels   = gt_labels\n",
    "\n",
    "def ask_openai(prompt: str) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Answer only with 'Yes' or 'No'.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "correct = 0\n",
    "predictions = []\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    answer = ask_openai(query).strip().lower()\n",
    "\n",
    "    if answer == \"yes.\":\n",
    "        pred_label = 1\n",
    "    elif answer == \"no.\":\n",
    "        pred_label = 0\n",
    "    else:\n",
    "        pred_label = -1  # Invalid response\n",
    "\n",
    "    predictions.append(pred_label)\n",
    "\n",
    "    gt_label = gt_labels[i]\n",
    "    is_correct = pred_label == gt_label\n",
    "\n",
    "    # Print result\n",
    "    print(f\"Q{i+1}: {query}\")\n",
    "    print(f\"→ Model: {answer} | GT: {'Yes' if gt_label else 'No'} | \"\n",
    "          f\"{'✅' if is_correct else ('❌ (Invalid)' if pred_label == -1 else '❌')}\")\n",
    "    print()\n",
    "\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / len(queries)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualititive Analysis\n",
    "\n",
    "This analysis involves manually reviewing the model's responses to assess clarity, accuracy, and alignment with legal context. It helps identify nuanced issues like misinterpretation of city-specific terms or inconsistent language use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muniscope\n",
    "\n",
    "queries = queries\n",
    "# Loop through each query and display its report\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\n{'='*80}\\nQuery {i}: {query}\\n\")\n",
    "    try:\n",
    "        report = Report(db, place, query)\n",
    "        display(Markdown(str(report)))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to generate report for Query {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI() \n",
    "\n",
    "queries = queries_x\n",
    "\n",
    "def ask_openai(prompt: str) -> str:\n",
    "    system_prompt = (\n",
    "        \"You are a legal analysis assistant. For each query, return a detailed answer citing the relevant municipal ordinance, \"\n",
    "        \"section, or paragraph if possible. If no reference is found, state that clearly. Be precise and quote text from the code if applicable.\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    answer = ask_openai(query)\n",
    "\n",
    "    print(f\"Q{i+1}: {query}\")\n",
    "    print(f\"→ Detailed Answer with Reference: {answer}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
